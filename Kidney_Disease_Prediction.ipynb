{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qsl744vNoNw",
        "outputId": "52b1bdc5-8f79-4d90-a382-cd2269bd90f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.925\n",
            "     Actual  Predicted\n",
            "161       0          0\n",
            "20        0          0\n",
            "392       2          2\n",
            "303       2          2\n",
            "339       2          2\n",
            "249       0          0\n",
            "53        0          0\n",
            "88        0          0\n",
            "260       2          2\n",
            "103       0          0\n",
            "75        0          0\n",
            "130       0          0\n",
            "387       2          2\n",
            "14        0          0\n",
            "291       2          2\n",
            "58        0          0\n",
            "188       0          0\n",
            "361       0          2\n",
            "175       0          0\n",
            "353       2          2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv(\"/content/kidney_disease.csv\")\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "df[num_cols] = df[num_cols].fillna(df[num_cols].mean())\n",
        "df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n",
        "\n",
        "# Step 3: Encode categorical columns\n",
        "encoder = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "# Step 4: Split into features (X) and target (y)\n",
        "X = df.drop('classification', axis=1)\n",
        "y = df['classification']\n",
        "\n",
        "# Step 5: Add small random noise to numeric columns\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    noise = np.random.normal(0, 0.5, size=X[col].shape)  # mean=0, std=0.05\n",
        "    X[col] = X[col] + noise\n",
        "\n",
        "# Flip 10% of the labels, but ensure they stay within the valid class range (0 or 2)\n",
        "np.random.seed(42)\n",
        "flip_idx = np.random.choice(y.index, size=int(0.1 * len(y)), replace=False)\n",
        "y_noisy = y.copy()\n",
        "# Flip labels between 0 and 2 only\n",
        "for idx in flip_idx:\n",
        "    y_noisy.loc[idx] = 2 if y_noisy.loc[idx] == 0 else 0\n",
        "\n",
        "# Step 6: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_noisy, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 7: Build and train the model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Check the accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Step 10: Show actual vs predicted\n",
        "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(results.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Step 2: Classification Report (includes precision, recall, f1-score)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Step 3: Precision, Recall, and F1-Score for each class\n",
        "precision = precision_score(y_test, y_pred, average=None)  # average=None gives precision for each class\n",
        "recall = recall_score(y_test, y_pred, average=None)  # average=None gives recall for each class\n",
        "f1 = f1_score(y_test, y_pred, average=None)  # average=None gives F1 score for each class\n",
        "\n",
        "print(\"\\nPrecision for each class:\")\n",
        "print(precision)\n",
        "\n",
        "print(\"\\nRecall for each class:\")\n",
        "print(recall)\n",
        "\n",
        "print(\"\\nF1-Score for each class:\")\n",
        "print(f1)\n",
        "\n",
        "# You can also compute macro and weighted averages if needed\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"\\nMacro Average Precision:\", precision_macro)\n",
        "print(\"Macro Average Recall:\", recall_macro)\n",
        "print(\"Macro Average F1-Score:\", f1_macro)\n",
        "\n",
        "# Weighted averages for handling imbalanced classes\n",
        "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
        "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
        "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"\\nWeighted Average Precision:\", precision_weighted)\n",
        "print(\"Weighted Average Recall:\", recall_weighted)\n",
        "print(\"Weighted Average F1-Score:\", f1_weighted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooxabCj7V4U7",
        "outputId": "eb8c98f2-540f-4566-9b85-5a500616a545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[46  2]\n",
            " [ 4 28]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94        48\n",
            "           2       0.93      0.88      0.90        32\n",
            "\n",
            "    accuracy                           0.93        80\n",
            "   macro avg       0.93      0.92      0.92        80\n",
            "weighted avg       0.93      0.93      0.92        80\n",
            "\n",
            "\n",
            "Precision for each class:\n",
            "[0.92       0.93333333]\n",
            "\n",
            "Recall for each class:\n",
            "[0.95833333 0.875     ]\n",
            "\n",
            "F1-Score for each class:\n",
            "[0.93877551 0.90322581]\n",
            "\n",
            "Macro Average Precision: 0.9266666666666667\n",
            "Macro Average Recall: 0.9166666666666667\n",
            "Macro Average F1-Score: 0.9210006583278473\n",
            "\n",
            "Weighted Average Precision: 0.9253333333333333\n",
            "Weighted Average Recall: 0.925\n",
            "Weighted Average F1-Score: 0.9245556287030942\n"
          ]
        }
      ]
    }
  ]
}